# Moviepedia

This is the course project of Realtime and Big Data Analytics @ NYU done by

* Xinyi Liu (`xl2700`)
* Xinsen Lu (`xl2783`)
* Yiming Li (`yl6183`)

## Data Ingest (`/data_ingest`)

### IMDb 5000 Dataset (`/data_ingest/imdb_5000`)

This IMDB 5000 Dataset is originally generated by Chuan Sun (https://github.com/sundeepblue/movie_rating_prediction).

We rerun the crawler with some minor modifications due to the change of structures from the website. We update information of 

* Box Office (Domestic Gross and Worldwide Gross)
* IMDb Rating with the number of people who vote
* IMDb Reviews Number from both users and critics

by running
```
scrapy crawl movie_budget -o movie_budget_2018.json
```
```
scrapy crawl fetch_imdb_url -o fetch_imdb_url_2018.json
```
```
scrapy crawl imdb -o imdb_output_2018.json
```

We are not able to crawl or get data from Facebook, so we choose to keep them, and update other fields. Therefore, We combine the following files, in the data cleaning and profiling stage.

* `movie_budget_2018.json`
* `fetch_imdb_url_2018.json`
* `imdb_output_2018.json`
* `imdb_output_2017.json` (the original dataset)

We move them to Dumbo by
```
scp *.json xl2700@dumbo.es.its.nyu.edu:~/class9
```
and then put them to HDFS by
```
hdfs dfs -put *.json class9
```

You may access the files through `/home/xl2700/class9/` from local file system or `/user/xl2700/class9/` from HDFS on Dumbo.

### IMDb Reviews Dataset (`/data_ingest/imdb_review`)

IMDb Reviews Dataset is scraped from IMDb websites by using a spider based on Scrapy Framework according to the list given by IMDb 5000 Dataset.
The spider is in folder `data_ingest/imdb_review`. We can run this spider in the Python 3.7 environment with module `scrapy` installed. The spider will read the text file `movie_metadata.csv` and return the result to a file called `reviews.txt`.

The process would be
```
scrapy crawl movieSpider
```

We will get the file `reviews.txt` and move it onto Dumbo by
```
scp reviews.txt dumbo:fp/clean
```
and then put it into HDFS by
```
mv reviews.txt raw.txt
hdfs dfs -put raw.txt /user/xl2783/clean/raw.txt
```

You may access the output file `raw.txt` through `/home/fp/clean/raw.txt` from local file system, or `/user/xl2783/clean/raw.txt` from HDFS on Dumbo.

### Twitter Reviews Dataset (`/data_ingest/twitter_review`)

This Twitter Reviews Dataset has tweets sharing reviews of movies listed in IMDb 5000 Dataset. 

We run the code file `ReadTweet.java` on local machine with an imput file `movie_info`, which has all the movie information from IMDb 5000 Dataset, and then output all tweets into `project_data.txt`.

We move it to Dumbo by
```
scp project_data.txt dumbo:~
```
and then put it into HDFS by
```
hdfs dfs -put project_data.txt /user/yl6183/project
```

You may access the input file `moive_info` through `/home/yl6183/movie_info` from local file system, or `/user/yl6183/project/movie_info` from HDFS on Dumbo.

You may access the output file `project_data.txt` through `/home/yl6183/project_data.txt` from local file system, or `/user/yl6183/project/project_data.txt` from HDFS on Dumbo.

## Data Profiling (`/profiling_code`)

### IMDb 5000 Dataset (`/profiling_code/imdb_5000`)

There are three MapReduce jobs

* `map1.py` and `reduce1.py`
* `map2.py` and `reduce2.py`
* `map3.py` and `reduce3.py`

to

* test whether each entry in `*.json` follows JSON format and can be read successfully
* test whether each search link maps to multiple movies in IMDb or budget information from The Numbers, drop all related if true, otherwise join them and update the fields
* test whether *facebook_likes*, *duration*, *aspect_ratio* exist and follow specific formats, parse them if true, otherwise fill in 0 or *NULL* values

The commands to launch the three jobs are
```
hadoop jar /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.reduces=1 -files ./map1.py,./reduce1.py -mapper "python map1.py" -reducer "python reduce1.py" -input /user/xl2700/class9/movie_budget_2018.json,/user/xl2700/class9/fetch_imdb_url_2018.json -output /user/xl2700/class9/output1
```
```
hadoop jar /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.reduces=1 -files ./map2.py,./reduce2.py -mapper "python map2.py" -reducer "python reduce2.py" -input /user/xl2700/class9/imdb_output_2018.json,/user/xl2700/class9/output1 -output /user/xl2700/class9/output2
```
```
hadoop jar /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.reduces=1 -files ./map3.py,./reduce3.py -mapper "python map3.py" -reducer "python reduce3.py" -input /user/xl2700/class9/imdb_output_2017.json,/user/xl2700/class9/output2 -output /user/xl2700/class9/output3
```

### IMDb Reviews Dataset (`/profiling_code/imdb_review`)

There are two tasks in one MapReduce job:

* find the score value range of all the reviews
* find the number of spoiler reviews in reviews

The command to launch the job is
```
hadoop jar profile.jar Profile /user/xl2783/profile/raw.txt /user/xl2783/clean/output
```

### Twitter Reviews Dataset (`/profiling_code/twitter_review`)

One MapReduce job is used to test the length of each tweet and the percentage of English characters inside each tweet.

The command to launch the job is
```
hadoop jar dataProfile.jar DataProfile /user/yl6183/project/project_data.txt /user/yl6183/project/profileOutput
```

## Data Cleaning (`/etl_code`)

### IMDb 5000 Dataset (`/etl_code/imdb_5000`)

There are three MapReduce jobs

* `map4.py`
* `map5.py` and `reduce5.py`
* `map6.py` and `reduce6.py`

to

* drop unnecessary columns
* construct separate tables (`movie`, `genres` and `actor`) rather than a collection of JSONs
* deal with non-English characters

The commands to launch the three jobs are
```
hadoop jar /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.maps=1 -D mapreduce.job.reduces=0 -files ./map4.py -mapper "python map4.py" -input /user/xl2700/class9/output3 -output /user/xl2700/class9/output4
```
```
hadoop jar /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.reduces=1 -files ./map5.py,./reduce5.py -mapper "python map5.py" -reducer "python reduce5.py" -input /user/xl2700/class9/output3 -output /user/xl2700/class9/output5
```
```
hadoop jar /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.reduces=1 -files ./map6.py,./reduce6.py -mapper "python map6.py" -reducer "python reduce6.py" -input /user/xl2700/class9/output3 -output /user/xl2700/class9/output6
```

### IMDb Reviews Dataset (`/etl_code/imdb_review`)

The stop words to remove come from https://sites.google.com/site/kevinbouge/stopwords-lists, and we use `stopwords_en.txt` here.

There is one MapReduce job

* `Clean.java`, `CleanMapper.java`, `CleanReducer.java`

to 

* remove the reviews that have no score made on the movie.
* remove the stop words that is meaningless to the final sentimental analysis by caching the stop words file into the mapreduce job.
* remove non-English words, user links and other characters in the reviews.
* add up the scores of the same word in the same movie with word counted.

The command to launch the job is
```
hadoop jar clean.jar Clean /user/xl2783/clean/raw.txt /user/xl2783/clean/output
```

You may access `stopwords_en.txt` through `/home/xl2783/fp/clean/stopwords_en.txt` from local file system, or `/user/xl2783/clean/stopwords_en.txt` from HDFS on Dumbo.

### Twitter Reviews Dataset (`/etl_code/twitter_review`)

There are two MapReduce jobs

* `DataClean.java`
* `SentiCountMap.py`, `SentiCountReduce.py`

to

* remove non-english words, useless links and `@` from the Twitter Reveiws Dataset, and assign each tweet with a movie key
* remove stop words, conduct sentiment analysis and count the number of times a word appears in a movie review, grouped by movie keys.

The command to launch them are
```
hadoop jar dataClean.jar DataClean /user/yl6183/project/project_data.txt /user/yl6183/project/cleanOutput -finds /user/yl6183/project/movie_info
```
```
hadoop jar /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.reduces=1 -files hdfs://dumbo/user/yl6183/project/python_code/SentiCountMap/py,hdfs://dumbo/user/yl6183/project/python_code/SentiCountReduce.py -cacheFile hdfs://dumbo/user/yl6183/project/stopwords_en.txt#english -cacheFile hdfs://dumbo/user/yl6183/project/SentiWords.txt#SentiWords.txt -mapper "python SentiCountMap.py" -reducer "python SentiCountReduce.py" -input /user/yl6183/project/cleanOutput/part-r-00000 -output /user/yl6183/project/SentiOutput
```

The `movie_info` file used in the first MapReduce job is the same as in data_ingest part. It is used to assign keys to tweets.

The stop words to remove come from https://sites.google.com/site/kevinbouge/stopwords-lists, and we use `stopwords_en.txt` in this project.

The dictionary for sentiment analysis come form https://hlt-nlp.fbk.eu/technologies/sentiwords, and we use `SentiWords.txt` in this project.

You may access `stopwords_en.txt` and `SentiWords.txt` through `/home/yl6183/stopwords_en.txt`, `/home/yl6183/SentiWords.txt` from local file system, or `/user/yl6183/project/stopwords_en.txt`, `/user/yl6183/SentiWords.txt` from HDFS on Dumbo.  

## Analytic Code (`/code_iterations`)

We use **Apache Hive** to run the queries in `/code_iterations/moviepedia.sql`. Please type the command
```
use xl2700;
```
in `beeline` to access the tables.

The external tables are stored at
* `movie`: `/user/xl2700/class9/output4/`
* `genres`: `/user/xl2700/class9/output5/`
* `actor`: `/user/xl2700/class9/output6/`
* `imdb_review`: `/user/xl2700/moviepedia/imdb/` (copied from `/user/xl2783/clean/output/`)
* `twitter_review`: `/user/xl2700/moviepedia/twitter/` (copied from `/user/yl6183/project/SentiOutput/`)

We mainly study
* the trend of the development in movie industry
* the key to the success of a movie by the correlation between each pair of numerical factors (stored in `correlation`)
* previous great works
* previous great works by genre
* characteristics of each genre
* top directors and actors/actresses within each genre to consider when forming a film crew (stored in `top_directors` and `top_actors`)
* top commonly mentioned words from reviews on IMDb and Twitter by genre (stored in `imdb_top_words` and `twitter_top_words`)

Unless other specified (stored in table), the results are directly read from the console.

## Screenshots (`/screenshots`)

The organization of the screenshots to prove this analytic running follows the organization of the codes.