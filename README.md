# Moviepedia

This is the course project of Realtime and Big Data Analytics @ NYU done by
* Xinyi Liu (`xl2700`)
* Xinsen Lu (`xl2783`)
* Yiming Li (`yl6183`)

## Data Ingest (`/data_ingest`)

### IMDb 5000 Dataset (`/data_ingest/imdb_5000`)

This IMDB 5000 Dataset is originally generated by Chuan Sun (https://github.com/sundeepblue/movie_rating_prediction).

We rerun the crawler with some minor modifications due to the change of structures from the website. We update information of 
* Box Office (Domestic Gross and Worldwide Gross)
* IMDb Rating with the number of people who vote
* IMDb Reviews Number from both users and critics
by running
```
scrapy crawl movie_budget -o movie_budget_2018.json
```
```
scrapy crawl fetch_imdb_url -o fetch_imdb_url_2018.json
```
```
scrapy crawl imdb -o imdb_output_2018.json
```

We are not able to crawl or get data from Facebook, so we choose to keep them, and update other fields. Therefore, We combine the following files, in the data cleaning and profiling stage.
* `movie_budget_2018.json`
* `fetch_imdb_url_2018.json`
* `imdb_output_2018.json`
* `imdb_output_2017.json` (the original dataset)

We move them to Dumbo by
```
scp *.json xl2700@dumbo.es.its.nyu.edu:~/class9
```
and then put them to HDFS by
```
hdfs dfs -put *.json class9
```

You may access the file through `/user/xl2700/class9/` from local file system or `/user/xl2700/class9/` from HDFS on Dumbo.

### IMDb Reviews Dataset (`/data_ingest/imdb_review`)

### Twitter Reviews Dataset (`/data_ingest/twitter_review`)

## Data Profiling (`/profiling_code`)

### IMDb 5000 Dataset (`/profiling_code/imdb_5000`)

There are three MapReduce jobs
* `map1.py` and `reduce1.py`
* `map2.py` and `reduce2.py`
* `map3.py` and `reduce3.py`
to
* test whether each entry in `*.json` follows JSON format and can be read successfully
* test whether each search link maps to multiple movies in IMDb or budget information from The Numbers, drop all related if true, otherwise join them and update the fields
* test whether *facebook_likes*, *duration*, *aspect_ratio* exist and follow specific formats, parse them if true, otherwise fill in 0 or *NULL* values

The commands to launch the three jobs are
```
hadoop jar /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.reduces=1 -files ./map1.py,./reduce1.py -mapper "python map1.py" -reducer "python reduce1.py" -input /user/xl2700/class9/movie_budget_2018.json,/user/xl2700/class9/fetch_imdb_url_2018.json -output /user/xl2700/class9/output1
```
```
hadoop jar /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.reduces=1 -files ./map2.py,./reduce2.py -mapper "python map2.py" -reducer "python reduce2.py" -input /user/xl2700/class9/imdb_output_2018.json,/user/xl2700/class9/output1 -output /user/xl2700/class9/output2
```
```
hadoop jar /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.reduces=1 -files ./map3.py,./reduce3.py -mapper "python map3.py" -reducer "python reduce3.py" -input /user/xl2700/class9/imdb_output_2017.json,/user/xl2700/class9/output2 -output /user/xl2700/class9/output3
```

### IMDb Reviews Dataset (`/profiling_code/imdb_review`)

### Twitter Reviews Dataset (`/profiling_code/twitter_review`)

## Data Cleaning (`/etl_code`)

### IMDb 5000 Dataset (`/etl_code/imdb_5000`)

There are three MapReduce jobs
* `map4.py`
* `map5.py` and `reduce5.py`
* `map6.py` and `reduce6.py`
to
* drop unnecessary columns
* construct separate tables (`movie`, `genres` and `actor`) rather than a collection of JSONs
* deal with non-English characters

The commands to launch the three jobs are
```
hadoop jar /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.maps=1 -D mapreduce.job.reduces=0 -files ./map4.py -mapper "python map4.py" -input /user/xl2700/class9/output3 -output /user/xl2700/class9/output4
```
```
hadoop jar /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.reduces=1 -files ./map5.py,./reduce5.py -mapper "python map5.py" -reducer "python reduce5.py" -input /user/xl2700/class9/output3 -output /user/xl2700/class9/output5
```
```
hadoop jar /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar -D mapreduce.job.reduces=1 -files ./map6.py,./reduce6.py -mapper "python map6.py" -reducer "python reduce6.py" -input /user/xl2700/class9/output3 -output /user/xl2700/class9/output6
```

### IMDb Reviews Dataset (`/etl_code/imdb_review`)

The stop words to remove come from https://sites.google.com/site/kevinbouge/stopwords-lists, and we use `stopwords_en.txt` here.

### Twitter Reviews Dataset (`/etl_code/twitter_review`)

The stop words to remove come from https://sites.google.com/site/kevinbouge/stopwords-lists, and we use `stopwords_en.txt` here.

## Analytic Code (`/code_iterations`)

We use **Apache Hive** to run the queries in `/code_iterations/moviepedia.sql`. Please type the command
```
use xl2700;
```
in `beeline` to access the tables.

The external tables are stored at
* `movie`: `/user/xl2700/class9/output4/`
* `genres`: `/user/xl2700/class9/output5/`
* `actor`: `/user/xl2700/class9/output6/`
* `imdb_review`: `/user/xl2700/moviepedia/imdb/` (copied from `/path`)
* `twitter_review`: `/user/xl2700/moviepedia/twitter/` (copied from `/path`)

We mainly study
* the trend of the development in movie industry
* the key to the success of a movie by the correlation between each pair of numerical factors (stored in `correlation`)
* previous great works
* previous great works by genre
* characteristics of each genre
* top directors and actors/actresses within each genre to consider when forming a film crew (stored in `top_directors` and `top_actors`)
* top commonly mentioned words from reviews on IMDb and Twitter by genre (stored in `imdb_top_words` and `twitter_top_words`)

Unless other specified (stored in table), the results are directly read from the console.

## Screenshots (`/screenshots`)

The organization of the screenshots to prove this analytic running follows the organization of the codes.